{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import ray\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "from typing import List\n",
    "from gym import spaces\n",
    "from ray.rllib.algorithms.dqn.distributional_q_tf_model import DistributionalQTFModel\n",
    "from ray.rllib.algorithms.apex_dqn import ApexDQN\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.utils.typing import ModelConfigDict, TensorType, AlgorithmConfigDict, EnvCreator\n",
    "from environments.knapsack import KnapsackEnv\n",
    "\n",
    "from ray.rllib.algorithms.dqn.dqn_tf_policy import DQNTFPolicy\n",
    "from ray.rllib.models.tf.tf_action_dist import get_categorical_class_with_temperature\n",
    "from ray.rllib.utils.tf_utils import reduce_mean_ignore_inf"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define Custom Model\n",
    "* Multiple Inputs are concatenated\n",
    "* Action Mask is applied"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "class DQNCustomModel(DistributionalQTFModel):\n",
    "    def __init__(\n",
    "            self,\n",
    "            obs_space: spaces.Space,\n",
    "            action_space: spaces.Space,\n",
    "            num_outputs: int,\n",
    "            model_config: ModelConfigDict,\n",
    "            name: str,\n",
    "            q_hiddens=(256,),\n",
    "            dueling=True,\n",
    "            num_atoms=51,\n",
    "            v_min=-1.0,\n",
    "            v_max=1.0,\n",
    "            use_noisy=True,\n",
    "            sigma0=0.5,\n",
    "            add_layer_norm=True,\n",
    "            verbose=True\n",
    "    ):\n",
    "        orig_space = getattr(obs_space, \"original_space\", obs_space)\n",
    "\n",
    "        assert (\n",
    "                isinstance(orig_space, spaces.Dict)\n",
    "                and \"action_mask\" in orig_space.spaces\n",
    "                and \"observations\" in orig_space.spaces\n",
    "        )\n",
    "\n",
    "        super().__init__(\n",
    "            obs_space,\n",
    "            action_space,\n",
    "            num_outputs,\n",
    "            model_config,\n",
    "            name,\n",
    "            q_hiddens=q_hiddens,\n",
    "            dueling=dueling,\n",
    "            num_atoms=num_atoms,\n",
    "            v_min=v_min,\n",
    "            v_max=v_max,\n",
    "            use_noisy=use_noisy,\n",
    "            sigma0=sigma0,\n",
    "            add_layer_norm=add_layer_norm\n",
    "        )\n",
    "        self._num_atoms = num_atoms\n",
    "\n",
    "        weight_inputs = tf.keras.layers.Input(shape=orig_space['observations']['observation_weights'].shape, name='observation_weights')\n",
    "        value_inputs = tf.keras.layers.Input(shape=orig_space['observations']['observation_values'].shape, name='observation_values')\n",
    "        knapsack_inputs = tf.keras.layers.Input(shape=orig_space['observations']['observation_knapsack'].shape, name='observation_knapsack')\n",
    "        inputs = tf.keras.layers.Concatenate(axis=-1)([weight_inputs, value_inputs, knapsack_inputs])\n",
    "        hidden = tf.keras.layers.Dense(units=256, activation='tanh', name='hidden')(inputs)\n",
    "        outputs = tf.keras.layers.Dense(units=num_outputs, activation='tanh', name='outputs')(hidden)\n",
    "        self.base_model = tf.keras.Model(inputs=[weight_inputs, value_inputs, knapsack_inputs], outputs=outputs, name='base_model')\n",
    "\n",
    "        self._action_mask = None\n",
    "\n",
    "        if verbose and dueling:\n",
    "            print('--- Value Network ---')\n",
    "            self.state_value_head.summary(expand_nested=True)\n",
    "\n",
    "            print('--- Q Network ---')\n",
    "            self.q_value_head.summary(expand_nested=True)\n",
    "\n",
    "    def forward(self, input_dict: dict[str, TensorType], state: list[TensorType], seq_lens: TensorType) -> (TensorType, list[TensorType]):\n",
    "        self._action_mask = input_dict['obs']['action_mask']\n",
    "        model_out = self.base_model(input_dict['obs']['observations'])\n",
    "        return model_out, state\n",
    "\n",
    "    def get_q_value_distributions(self, model_out: TensorType) -> List[TensorType]:\n",
    "        q_values_out = super().get_q_value_distributions(model_out=model_out)\n",
    "        inf_mask = tf.maximum(tf.math.log(self._action_mask), tf.float32.min)\n",
    "\n",
    "        if self._num_atoms == 1:\n",
    "            action_scores, logits, dist = q_values_out\n",
    "            action_scores += inf_mask\n",
    "            return [action_scores, logits, dist]\n",
    "        else:\n",
    "            action_scores, z, support_logits_per_action, logits, dist = q_values_out\n",
    "            return [\n",
    "                action_scores + inf_mask,\n",
    "                z,\n",
    "                support_logits_per_action + tf.expand_dims(inf_mask, axis=-1),\n",
    "                logits,\n",
    "                dist\n",
    "            ]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train Custom Masked Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 19:08:16,599\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "2023-01-06 19:08:17,806\tWARNING deprecation.py:47 -- DeprecationWarning: `algo = Algorithm(env='<class 'environments.knapsack.KnapsackEnv'>', ...)` has been deprecated. Use `algo = AlgorithmConfig().environment('<class 'environments.knapsack.KnapsackEnv'>').build()` instead. This will raise an error in the future!\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=13316)\u001B[0m 2023-01-06 19:08:22,290\tWARNING env.py:147 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2023-01-06 19:08:31,660\tINFO trainable.py:172 -- Trainable.setup took 13.849 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2023-01-06 19:08:31,662\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"base_model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " observation_weights (InputLaye  [(None, 10)]        0           []                               \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " observation_values (InputLayer  [(None, 10)]        0           []                               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " observation_knapsack (InputLay  [(None, 1)]         0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 21)           0           ['observation_weights[0][0]',    \n",
      "                                                                  'observation_values[0][0]',     \n",
      "                                                                  'observation_knapsack[0][0]']   \n",
      "                                                                                                  \n",
      " hidden (Dense)                 (None, 256)          5632        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 256)          65792       ['hidden[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 71,424\n",
      "Trainable params: 71,424\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(MultiAgentPrioritizedReplayBuffer pid=19312)\u001B[0m 2023-01-06 19:08:31,779\tINFO replay_buffer.py:63 -- Estimated max memory usage for replay buffer is 0.0035625 GB (12500.0 batches of size 1, 285 bytes each), available system memory is 34.160103424 GB\n",
      "\u001B[2m\u001B[36m(MultiAgentPrioritizedReplayBuffer pid=19312)\u001B[0m 2023-01-06 19:08:31,780\tWARNING multi_agent_prioritized_replay_buffer.py:215 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n",
      "\u001B[2m\u001B[36m(MultiAgentPrioritizedReplayBuffer pid=13368)\u001B[0m 2023-01-06 19:08:31,834\tINFO replay_buffer.py:63 -- Estimated max memory usage for replay buffer is 0.0035625 GB (12500.0 batches of size 1, 285 bytes each), available system memory is 34.160103424 GB\n",
      "\u001B[2m\u001B[36m(MultiAgentPrioritizedReplayBuffer pid=13368)\u001B[0m 2023-01-06 19:08:31,834\tWARNING multi_agent_prioritized_replay_buffer.py:215 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n",
      "\u001B[2m\u001B[36m(MultiAgentPrioritizedReplayBuffer pid=18604)\u001B[0m 2023-01-06 19:08:31,851\tINFO replay_buffer.py:63 -- Estimated max memory usage for replay buffer is 0.0035625 GB (12500.0 batches of size 1, 285 bytes each), available system memory is 34.160103424 GB\n",
      "\u001B[2m\u001B[36m(MultiAgentPrioritizedReplayBuffer pid=18604)\u001B[0m 2023-01-06 19:08:31,851\tWARNING multi_agent_prioritized_replay_buffer.py:215 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n",
      "\u001B[2m\u001B[36m(MultiAgentPrioritizedReplayBuffer pid=5216)\u001B[0m 2023-01-06 19:08:31,920\tINFO replay_buffer.py:63 -- Estimated max memory usage for replay buffer is 0.0035625 GB (12500.0 batches of size 1, 285 bytes each), available system memory is 34.160103424 GB\n",
      "\u001B[2m\u001B[36m(MultiAgentPrioritizedReplayBuffer pid=5216)\u001B[0m 2023-01-06 19:08:31,920\tWARNING multi_agent_prioritized_replay_buffer.py:215 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n",
      "\u001B[2m\u001B[36m(MultiAgentPrioritizedReplayBuffer pid=13368)\u001B[0m 2023-01-06 19:09:01,634\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n",
      "\u001B[2m\u001B[36m(MultiAgentPrioritizedReplayBuffer pid=19312)\u001B[0m 2023-01-06 19:09:02,432\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n",
      "\u001B[2m\u001B[36m(MultiAgentPrioritizedReplayBuffer pid=18604)\u001B[0m 2023-01-06 19:09:02,832\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n",
      "\u001B[2m\u001B[36m(MultiAgentPrioritizedReplayBuffer pid=5216)\u001B[0m 2023-01-06 19:09:05,244\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Average Returns: 20.2\n",
      "Iteration: 1, Average Returns: 26.0\n",
      "Iteration: 2, Average Returns: 25.6\n",
      "Iteration: 3, Average Returns: 26.0\n",
      "Iteration: 4, Average Returns: 26.0\n",
      "Iteration: 5, Average Returns: 26.0\n",
      "Iteration: 6, Average Returns: 26.0\n",
      "Iteration: 7, Average Returns: 26.0\n",
      "Iteration: 8, Average Returns: 26.0\n",
      "Iteration: 9, Average Returns: 26.0\n",
      "Iteration: 10, Average Returns: 26.0\n",
      "Iteration: 11, Average Returns: 26.0\n",
      "Iteration: 12, Average Returns: 26.0\n",
      "Iteration: 13, Average Returns: 26.0\n",
      "Iteration: 14, Average Returns: 26.0\n",
      "Iteration: 15, Average Returns: 25.6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [46]\u001B[0m, in \u001B[0;36m<cell line: 43>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      6\u001B[0m agent \u001B[38;5;241m=\u001B[39m ApexDQN(env\u001B[38;5;241m=\u001B[39mKnapsackEnv, config\u001B[38;5;241m=\u001B[39m{\n\u001B[0;32m      7\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124menv_config\u001B[39m\u001B[38;5;124m'\u001B[39m: {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mverbose\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;28;01mFalse\u001B[39;00m},\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnum_workers\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m2\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     40\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnum_gpus\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     41\u001B[0m })\n\u001B[0;32m     42\u001B[0m agent\u001B[38;5;241m.\u001B[39mget_policy()\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mbase_model\u001B[38;5;241m.\u001B[39msummary(expand_nested\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m---> 43\u001B[0m \u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43magent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43magent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meval_env\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mKnapsackEnv\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mverbose\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\Research\\others\\RLLib\\utils.py:37\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(agent, eval_env, train_iterations, iterations_per_eval, eval_iterations, plot_training, algo_name)\u001B[0m\n\u001B[0;32m     34\u001B[0m average_returns \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     36\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(train_iterations):\n\u001B[1;32m---> 37\u001B[0m     \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     39\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m%\u001B[39m iterations_per_eval \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     40\u001B[0m         average_return \u001B[38;5;241m=\u001B[39m evaluate(agent, eval_env, eval_iterations\u001B[38;5;241m=\u001B[39meval_iterations, render\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\tune\\trainable\\trainable.py:364\u001B[0m, in \u001B[0;36mTrainable.train\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    362\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m    363\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 364\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    365\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    366\u001B[0m     skipped \u001B[38;5;241m=\u001B[39m skip_exceptions(e)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:749\u001B[0m, in \u001B[0;36mAlgorithm.step\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    741\u001B[0m     (\n\u001B[0;32m    742\u001B[0m         results,\n\u001B[0;32m    743\u001B[0m         train_iter_ctx,\n\u001B[0;32m    744\u001B[0m     ) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_one_training_iteration_and_evaluation_in_parallel()\n\u001B[0;32m    745\u001B[0m \u001B[38;5;66;03m# - No evaluation necessary, just run the next training iteration.\u001B[39;00m\n\u001B[0;32m    746\u001B[0m \u001B[38;5;66;03m# - We have to evaluate in this training iteration, but no parallelism ->\u001B[39;00m\n\u001B[0;32m    747\u001B[0m \u001B[38;5;66;03m#   evaluate after the training iteration is entirely done.\u001B[39;00m\n\u001B[0;32m    748\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 749\u001B[0m     results, train_iter_ctx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_one_training_iteration\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    751\u001B[0m \u001B[38;5;66;03m# Sequential: Train (already done above), then evaluate.\u001B[39;00m\n\u001B[0;32m    752\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m evaluate_this_iter \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mevaluation_parallel_to_training\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\rllib\\algorithms\\algorithm.py:2623\u001B[0m, in \u001B[0;36mAlgorithm._run_one_training_iteration\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   2621\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timers[TRAINING_ITERATION_TIMER]:\n\u001B[0;32m   2622\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39m_disable_execution_plan_api:\n\u001B[1;32m-> 2623\u001B[0m         results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2624\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   2625\u001B[0m         results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_exec_impl)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\rllib\\algorithms\\apex_dqn\\apex_dqn.py:383\u001B[0m, in \u001B[0;36mApexDQN.training_step\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    381\u001B[0m \u001B[38;5;129m@override\u001B[39m(DQN)\n\u001B[0;32m    382\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtraining_step\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResultDict:\n\u001B[1;32m--> 383\u001B[0m     num_samples_ready \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_samples_and_store_to_replay_buffers\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    384\u001B[0m     num_worker_samples_collected \u001B[38;5;241m=\u001B[39m defaultdict(\u001B[38;5;28mint\u001B[39m)\n\u001B[0;32m    386\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m worker_id, samples_info \u001B[38;5;129;01min\u001B[39;00m num_samples_ready:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\rllib\\algorithms\\apex_dqn\\apex_dqn.py:469\u001B[0m, in \u001B[0;36mApexDQN.get_samples_and_store_to_replay_buffers\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    464\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_timers[SAMPLE_TIMER]:\n\u001B[0;32m    465\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mworkers\u001B[38;5;241m.\u001B[39mforeach_worker_async(\n\u001B[0;32m    466\u001B[0m         func\u001B[38;5;241m=\u001B[39mremote_worker_sample_and_store,\n\u001B[0;32m    467\u001B[0m         healthy_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m    468\u001B[0m     )\n\u001B[1;32m--> 469\u001B[0m     num_samples_ready \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mworkers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch_ready_async_reqs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    470\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout_seconds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sample_req_tiemeout_s\u001B[49m\n\u001B[0;32m    471\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    472\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m num_samples_ready\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\rllib\\evaluation\\worker_set.py:796\u001B[0m, in \u001B[0;36mWorkerSet.fetch_ready_async_reqs\u001B[1;34m(self, timeout_seconds, return_obj_refs)\u001B[0m\n\u001B[0;32m    779\u001B[0m \u001B[38;5;129m@DeveloperAPI\u001B[39m\n\u001B[0;32m    780\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch_ready_async_reqs\u001B[39m(\n\u001B[0;32m    781\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    784\u001B[0m     return_obj_refs: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    785\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[Tuple[\u001B[38;5;28mint\u001B[39m, T]]:\n\u001B[0;32m    786\u001B[0m     \u001B[38;5;124;03m\"\"\"Get esults from outstanding asynchronous requests that are ready.\u001B[39;00m\n\u001B[0;32m    787\u001B[0m \n\u001B[0;32m    788\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    794\u001B[0m \u001B[38;5;124;03m        paired with the indices of the callee workers.\u001B[39;00m\n\u001B[0;32m    795\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 796\u001B[0m     remote_results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__worker_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch_ready_async_reqs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    797\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout_seconds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout_seconds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    798\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_obj_refs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_obj_refs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    801\u001B[0m     handle_remote_call_result_errors(remote_results, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ignore_worker_failures)\n\u001B[0;32m    803\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [(r\u001B[38;5;241m.\u001B[39mactor_id, r\u001B[38;5;241m.\u001B[39mget()) \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m remote_results\u001B[38;5;241m.\u001B[39mignore_errors()]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py:681\u001B[0m, in \u001B[0;36mFaultTolerantActorManager.fetch_ready_async_reqs\u001B[1;34m(self, timeout_seconds, return_obj_refs)\u001B[0m\n\u001B[0;32m    666\u001B[0m \u001B[38;5;124;03m\"\"\"Get results from outstanding async requests that are ready.\u001B[39;00m\n\u001B[0;32m    667\u001B[0m \n\u001B[0;32m    668\u001B[0m \u001B[38;5;124;03mAutomatically mark actors unhealthy if they fail to respond.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    678\u001B[0m \u001B[38;5;124;03m    remote call in the format of RemoteCallResults.\u001B[39;00m\n\u001B[0;32m    679\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    680\u001B[0m \u001B[38;5;66;03m# Construct the list of in-flight requests.\u001B[39;00m\n\u001B[1;32m--> 681\u001B[0m ready, remote_results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__fetch_result\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    682\u001B[0m \u001B[43m    \u001B[49m\u001B[43mremote_actor_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__in_flight_req_to_actor_id\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    683\u001B[0m \u001B[43m    \u001B[49m\u001B[43mremote_calls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__in_flight_req_to_actor_id\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeys\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    684\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimeout_seconds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout_seconds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    685\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_obj_refs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_obj_refs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    686\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    688\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m obj_ref, result \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(ready, remote_results):\n\u001B[0;32m    689\u001B[0m     \u001B[38;5;66;03m# Decrease outstanding request on this actor by 1.\u001B[39;00m\n\u001B[0;32m    690\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__remote_actor_states[\n\u001B[0;32m    691\u001B[0m         result\u001B[38;5;241m.\u001B[39mactor_id\n\u001B[0;32m    692\u001B[0m     ]\u001B[38;5;241m.\u001B[39mnum_in_flight_async_requests \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\rllib\\utils\\actor_manager.py:473\u001B[0m, in \u001B[0;36mFaultTolerantActorManager.__fetch_result\u001B[1;34m(self, remote_actor_ids, remote_calls, timeout_seconds, return_obj_refs)\u001B[0m\n\u001B[0;32m    470\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m\n\u001B[0;32m    472\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 473\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mray\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    474\u001B[0m     remote_results\u001B[38;5;241m.\u001B[39madd_result(actor_id, ResultOrError(result\u001B[38;5;241m=\u001B[39mresult))\n\u001B[0;32m    475\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    476\u001B[0m     \u001B[38;5;66;03m# Return error to the user.\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\_private\\client_mode_hook.py:105\u001B[0m, in \u001B[0;36mclient_mode_hook.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    103\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minit\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m is_client_mode_enabled_by_default:\n\u001B[0;32m    104\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(ray, func\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 105\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\_private\\worker.py:2303\u001B[0m, in \u001B[0;36mget\u001B[1;34m(object_refs, timeout)\u001B[0m\n\u001B[0;32m   2298\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   2299\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mobject_refs\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m must either be an ObjectRef or a list of ObjectRefs.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2300\u001B[0m     )\n\u001B[0;32m   2302\u001B[0m \u001B[38;5;66;03m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001B[39;00m\n\u001B[1;32m-> 2303\u001B[0m values, debugger_breakpoint \u001B[38;5;241m=\u001B[39m \u001B[43mworker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_objects\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobject_refs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2304\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, value \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(values):\n\u001B[0;32m   2305\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value, RayError):\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\ray\\_private\\worker.py:668\u001B[0m, in \u001B[0;36mWorker.get_objects\u001B[1;34m(self, object_refs, timeout)\u001B[0m\n\u001B[0;32m    662\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m    663\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAttempting to call `get` on the value \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mobject_ref\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    664\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwhich is not an ray.ObjectRef.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    665\u001B[0m         )\n\u001B[0;32m    667\u001B[0m timeout_ms \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m) \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 668\u001B[0m data_metadata_pairs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcore_worker\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_objects\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    669\u001B[0m \u001B[43m    \u001B[49m\u001B[43mobject_refs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcurrent_task_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout_ms\u001B[49m\n\u001B[0;32m    670\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    671\u001B[0m debugger_breakpoint \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    672\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m (data, metadata) \u001B[38;5;129;01min\u001B[39;00m data_metadata_pairs:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init()\n",
    "ModelCatalog.register_custom_model(\"dqn_model\", DQNCustomModel)\n",
    "tf.random.set_seed(seed=0)\n",
    "random.seed(0)\n",
    "agent = ApexDQN(env=KnapsackEnv, config={\n",
    "    'env_config': {'verbose': False},\n",
    "    'num_workers': 2,\n",
    "    'model': {\n",
    "        'custom_model': 'dqn_model',\n",
    "        'custom_model_config': {}\n",
    "    },\n",
    "    'replay_buffer_config' : {\n",
    "        \"type\": 'MultiAgentPrioritizedReplayBuffer',\n",
    "        \"capacity\": 50000,\n",
    "        'prioritized_replay': True,\n",
    "        'prioritized_replay_alpha': 0.6,\n",
    "        'prioritized_replay_beta': 0.4,\n",
    "        'prioritized_replay_eps': 1e-6,\n",
    "        'replay_sequence_length': 1,\n",
    "    },\n",
    "    'num_steps_sampled_before_learning_starts': 10000,\n",
    "    'target_network_update_freq': 10000,\n",
    "    'rollout_fragment_length': 4,\n",
    "    'train_batch_size': 128,\n",
    "    'n_step': 3,\n",
    "    'double_q': True,\n",
    "    'dueling': False,\n",
    "    'noisy': True,\n",
    "    'num_atoms': 51,\n",
    "    'v_min': 1.0,\n",
    "    'v_max': 30.0,\n",
    "    'exploration_config': {\n",
    "        'epsilon_timesteps': 2,\n",
    "        'final_epsilon': 0.0\n",
    "    },\n",
    "    'seed': 0,\n",
    "    'gamma': 0.99,\n",
    "    'lr': 0.0005,\n",
    "    'num_gpus': 1\n",
    "})\n",
    "agent.get_policy().model.base_model.summary(expand_nested=True)\n",
    "utils.train(agent=agent, eval_env=KnapsackEnv(env_config={'verbose': False}))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
