{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import ray\n",
    "import tensorflow as tf\n",
    "from gym import spaces\n",
    "from ray.rllib.algorithms.ppo import PPO\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.policy.rnn_sequencing import add_time_dimension\n",
    "from ray.rllib.models.tf.tf_modelv2 import TFModelV2\n",
    "from environments.cartpole import MaskedCartPoleEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Custom LSTM Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LSTMPPOModel(TFModelV2):\n",
    "    def __init__(self, obs_space, action_space, num_outputs, model_config, name):\n",
    "        super().__init__(\n",
    "            obs_space=obs_space,\n",
    "            action_space=action_space,\n",
    "            num_outputs=num_outputs,\n",
    "            model_config=model_config,\n",
    "            name=name\n",
    "        )\n",
    "\n",
    "        orig_space = getattr(obs_space, \"original_space\", obs_space)\n",
    "\n",
    "        assert (\n",
    "                isinstance(orig_space, spaces.Dict)\n",
    "                and \"action_mask\" in orig_space.spaces\n",
    "                and \"observations\" in orig_space.spaces\n",
    "        )\n",
    "\n",
    "        self._cell_size = 64\n",
    "        self.num_outputs = 2\n",
    "\n",
    "        inputs = tf.keras.layers.Input(shape=orig_space['observations']['cartpole_obs'].shape, name='cartpole_obs')\n",
    "        x = tf.keras.layers.Dense(units=256, activation='tanh', name='hidden_1')(inputs)\n",
    "        flat_output = tf.keras.layers.Dense(units=256, activation='tanh', name='hidden_2')(x)\n",
    "        self.base_model = tf.keras.Model(inputs=inputs, outputs=flat_output, name='base_model')\n",
    "\n",
    "        lstm_input = tf.keras.layers.Input(shape=(None, 256), name='inputs')\n",
    "        state_in_h = tf.keras.layers.Input(shape=(self._cell_size,), name='h')\n",
    "        state_in_c = tf.keras.layers.Input(shape=(self._cell_size, ), name='c')\n",
    "        seq_in = tf.keras.layers.Input(shape=(), name='seq_in', dtype=tf.int32)\n",
    "\n",
    "        lstm_out, state_h, state_c = tf.keras.layers.LSTM(self._cell_size, return_sequences=True, return_state=True, name=\"lstm\")(\n",
    "            inputs=lstm_input,\n",
    "            mask=tf.sequence_mask(seq_in),\n",
    "            initial_state=[state_in_h, state_in_c]\n",
    "        )\n",
    "        policy_out = tf.keras.layers.Dense(units=self.num_outputs, activation=None, name='policy_out')(lstm_out)\n",
    "        value_out = tf.keras.layers.Dense(units=1, activation=None, name='value_out')(lstm_out)\n",
    "        self.rnn_model = tf.keras.Model(\n",
    "            inputs=[lstm_input, seq_in, state_in_h, state_in_c],\n",
    "            outputs=[policy_out, value_out, state_h, state_c],\n",
    "            name='lstm_model'\n",
    "        )\n",
    "\n",
    "    def forward(self, input_dict, state, seq_lens):\n",
    "        model_out, _ = self.forward_rnn(inputs=input_dict['obs']['observations'], state=state, seq_lens=seq_lens)\n",
    "\n",
    "        action_mask = input_dict['obs']['action_mask']\n",
    "        inf_mask = tf.maximum(tf.math.log(action_mask), tf.float32.min)\n",
    "        return tf.reshape(model_out, [-1, self.num_outputs]) + inf_mask, state\n",
    "\n",
    "    def forward_rnn(self, inputs, state, seq_lens):\n",
    "        x = self.base_model(inputs)\n",
    "        x = add_time_dimension(padded_inputs=x, seq_lens=seq_lens, framework='tf')\n",
    "\n",
    "        model_out, self._value_out, h, c = self.rnn_model([x, seq_lens] + state)\n",
    "        return model_out, [h, c]\n",
    "\n",
    "    def get_initial_state(self) -> list:\n",
    "        return [\n",
    "            np.zeros(self._cell_size, np.float32),\n",
    "            np.zeros(self._cell_size, np.float32)\n",
    "        ]\n",
    "\n",
    "    def value_function(self):\n",
    "        return tf.reshape(self._value_out, [-1])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train LSTM PPO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate(agent, eval_env, eval_iterations, render):\n",
    "    total_returns = 0.0\n",
    "\n",
    "    for _ in range(eval_iterations):\n",
    "        done = False\n",
    "        state = eval_env.reset()\n",
    "        policy_state = agent.get_policy().get_initial_state()\n",
    "\n",
    "        if render:\n",
    "            eval_env.render()\n",
    "\n",
    "        while not done:\n",
    "            action, policy_state, _ = agent.compute_single_action(observation=state, state=policy_state)\n",
    "            state, reward, done, _ = eval_env.step(action)\n",
    "\n",
    "            total_returns += reward\n",
    "\n",
    "            if render:\n",
    "                eval_env.render()\n",
    "    return total_returns / eval_iterations\n",
    "\n",
    "\n",
    "def train(\n",
    "        agent,\n",
    "        eval_env,\n",
    "        train_iterations=50,\n",
    "        iterations_per_eval=1,\n",
    "        eval_iterations=5,\n",
    "        plot_training=True,\n",
    "        algo_name='Agent'\n",
    "):\n",
    "    average_returns = []\n",
    "\n",
    "    for i in range(train_iterations):\n",
    "        agent.train()\n",
    "\n",
    "        if i % iterations_per_eval == 0:\n",
    "            average_return = evaluate(agent, eval_env, eval_iterations=eval_iterations, render=False)\n",
    "            average_returns.append(average_return)\n",
    "\n",
    "            print(f'Iteration: {i}, Average Returns: {average_return}')\n",
    "\n",
    "    if plot_training:\n",
    "        plt.plot(average_returns)\n",
    "        plt.title(f'{algo_name} Training Progress on CartPole')\n",
    "        plt.xlabel('Iterations')\n",
    "        plt.ylabel('Average Return')\n",
    "        plt.show()\n",
    "    return average_returns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build Agent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "ray.init()\n",
    "\n",
    "tf.random.set_seed(seed=0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "ModelCatalog.register_custom_model('lstm_model', LSTMPPOModel)\n",
    "\n",
    "ppo_agent = PPO(env=MaskedCartPoleEnv, config={\n",
    "    'model': {\n",
    "        'vf_share_layers': True,\n",
    "        'custom_model': 'ppo_model',\n",
    "        'custom_model_config': {},\n",
    "        'max_seq_len': 20\n",
    "    },\n",
    "    'render_env': True,\n",
    "    'num_workers': 1,\n",
    "    'rollout_fragment_length': 256,\n",
    "    'num_envs_per_worker': 1,\n",
    "    'batch_mode': 'complete_episodes',\n",
    "    'use_critic': True,\n",
    "    'use_gae': True,\n",
    "    'lambda': 0.95,\n",
    "    'clip_param': 0.3,\n",
    "    'kl_coeff': 0.2,\n",
    "    'entropy_coeff': 0.01,\n",
    "    'kl_target': 0.01,\n",
    "    'vf_loss_coeff': 0.5,\n",
    "    'shuffle_sequences': True,\n",
    "    'num_sgd_iter': 40,\n",
    "    'sgd_minibatch_size': 32,\n",
    "    'train_batch_size': 512,\n",
    "    'seed': 0,\n",
    "    'gamma': 0.99,\n",
    "    'lr': 0.0005,\n",
    "    'num_gpus': 1\n",
    "})\n",
    "\n",
    "ppo_agent.get_policy().model.base_model.summary(expand_nested=True)\n",
    "ppo_agent.get_policy().model.rnn_model.summary(expand_nested=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train(agent=ppo_agent, eval_env=MaskedCartPoleEnv(env_config={}))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
