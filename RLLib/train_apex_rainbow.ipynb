{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import ray\n",
    "import tensorflow as tf\n",
    "import utils\n",
    "from gym import spaces\n",
    "from ray.rllib.algorithms.dqn.distributional_q_tf_model import DistributionalQTFModel\n",
    "from ray.rllib.algorithms.apex_dqn import ApexDQN\n",
    "from ray.rllib.models import ModelCatalog\n",
    "from ray.rllib.utils.typing import ModelConfigDict, TensorType\n",
    "from environments.cartpole import CartPoleEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Custom Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class DQNCustomModel(DistributionalQTFModel):\n",
    "    def __init__(\n",
    "            self,\n",
    "            obs_space: spaces.Space,\n",
    "            action_space: spaces.Space,\n",
    "            num_outputs: int,\n",
    "            model_config: ModelConfigDict,\n",
    "            name: str,\n",
    "            q_hiddens=(256,),\n",
    "            dueling=True,\n",
    "            num_atoms=51,\n",
    "            v_min=-1.0,\n",
    "            v_max=1.0,\n",
    "            use_noisy=True,\n",
    "            sigma0=0.5,\n",
    "            add_layer_norm=True,\n",
    "            verbose=True\n",
    "    ):\n",
    "        super().__init__(\n",
    "            obs_space,\n",
    "            action_space,\n",
    "            num_outputs,\n",
    "            model_config,\n",
    "            name,\n",
    "            q_hiddens=q_hiddens,\n",
    "            dueling=dueling,\n",
    "            num_atoms=num_atoms,\n",
    "            v_min=v_min,\n",
    "            v_max=v_max,\n",
    "            use_noisy=use_noisy,\n",
    "            sigma0=sigma0,\n",
    "            add_layer_norm=add_layer_norm\n",
    "        )\n",
    "\n",
    "        self.base_model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=obs_space.shape, name='observations'),\n",
    "            tf.keras.layers.Dense(units=256, activation='tanh', name='hidden'),\n",
    "            tf.keras.layers.Dense(units=num_outputs, activation='tanh', name='outputs')\n",
    "        ], name='base_model')\n",
    "\n",
    "        if verbose and dueling:\n",
    "            print('--- Value Network ---')\n",
    "            self.state_value_head.summary(expand_nested=True)\n",
    "\n",
    "            print('--- Q Network ---')\n",
    "            self.q_value_head.summary(expand_nested=True)\n",
    "\n",
    "    def forward(self, input_dict: dict[str, TensorType], state: list[TensorType], seq_lens: TensorType) -> (TensorType, list[TensorType]):\n",
    "        model_out = self.base_model(input_dict['obs'])\n",
    "        return model_out, state"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Default Model Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 15:26:52,858\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "2023-01-06 15:26:54,179\tWARNING deprecation.py:47 -- DeprecationWarning: `algo = Algorithm(env='<class 'environments.cartpole.CartPoleEnv'>', ...)` has been deprecated. Use `algo = AlgorithmConfig().environment('<class 'environments.cartpole.CartPoleEnv'>').build()` instead. This will raise an error in the future!\n",
      "2023-01-06 15:26:54,180\tINFO algorithm_config.py:2503 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "2023-01-06 15:26:54,203\tINFO algorithm.py:501 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001B[2m\u001B[36m(RolloutWorker pid=10928)\u001B[0m 2023-01-06 15:26:59,122\tWARNING env.py:147 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2023-01-06 15:27:14,931\tINFO trainable.py:172 -- Trainable.setup took 20.729 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2023-01-06 15:27:14,932\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " observations (InputLayer)      [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " fc_1 (Dense)                   (None, 256)          1280        ['observations[0][0]']           \n",
      "                                                                                                  \n",
      " fc_out (Dense)                 (None, 256)          65792       ['fc_1[0][0]']                   \n",
      "                                                                                                  \n",
      " value_out (Dense)              (None, 1)            257         ['fc_1[0][0]']                   \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 67,329\n",
      "Trainable params: 67,329\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[2m\u001B[36m(MultiAgentPrioritizedReplayBuffer pid=19712)\u001B[0m 2023-01-06 15:27:15,269\tINFO replay_buffer.py:63 -- Estimated max memory usage for replay buffer is 0.001725 GB (25000.0 batches of size 1, 69 bytes each), available system memory is 34.160103424 GB\n",
      "\u001B[2m\u001B[36m(MultiAgentPrioritizedReplayBuffer pid=19712)\u001B[0m 2023-01-06 15:27:15,269\tWARNING multi_agent_prioritized_replay_buffer.py:215 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n",
      "\u001B[2m\u001B[36m(MultiAgentPrioritizedReplayBuffer pid=5168)\u001B[0m 2023-01-06 15:27:15,208\tINFO replay_buffer.py:63 -- Estimated max memory usage for replay buffer is 0.001725 GB (25000.0 batches of size 1, 69 bytes each), available system memory is 34.160103424 GB\n",
      "\u001B[2m\u001B[36m(MultiAgentPrioritizedReplayBuffer pid=5168)\u001B[0m 2023-01-06 15:27:15,209\tWARNING multi_agent_prioritized_replay_buffer.py:215 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n",
      "\u001B[2m\u001B[36m(MultiAgentPrioritizedReplayBuffer pid=3408)\u001B[0m 2023-01-06 15:27:15,298\tINFO replay_buffer.py:63 -- Estimated max memory usage for replay buffer is 0.001725 GB (25000.0 batches of size 1, 69 bytes each), available system memory is 34.160103424 GB\n",
      "\u001B[2m\u001B[36m(MultiAgentPrioritizedReplayBuffer pid=3408)\u001B[0m 2023-01-06 15:27:15,299\tWARNING multi_agent_prioritized_replay_buffer.py:215 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n",
      "\u001B[2m\u001B[36m(MultiAgentPrioritizedReplayBuffer pid=11516)\u001B[0m 2023-01-06 15:27:15,318\tINFO replay_buffer.py:63 -- Estimated max memory usage for replay buffer is 0.001725 GB (25000.0 batches of size 1, 69 bytes each), available system memory is 34.160103424 GB\n",
      "\u001B[2m\u001B[36m(MultiAgentPrioritizedReplayBuffer pid=11516)\u001B[0m 2023-01-06 15:27:15,319\tWARNING multi_agent_prioritized_replay_buffer.py:215 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n",
      "\u001B[2m\u001B[36m(MultiAgentPrioritizedReplayBuffer pid=11516)\u001B[0m 2023-01-06 15:27:40,272\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n",
      "\u001B[2m\u001B[36m(MultiAgentPrioritizedReplayBuffer pid=5168)\u001B[0m 2023-01-06 15:27:41,503\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n",
      "\u001B[2m\u001B[36m(MultiAgentPrioritizedReplayBuffer pid=3408)\u001B[0m 2023-01-06 15:27:42,330\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n",
      "\u001B[2m\u001B[36m(MultiAgentPrioritizedReplayBuffer pid=19712)\u001B[0m 2023-01-06 15:27:46,070\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Average Returns: 11.0\n",
      "Iteration: 1, Average Returns: 23.4\n",
      "Iteration: 2, Average Returns: 22.8\n",
      "Iteration: 3, Average Returns: 102.0\n",
      "Iteration: 4, Average Returns: 193.4\n",
      "Iteration: 5, Average Returns: 127.8\n",
      "Iteration: 6, Average Returns: 162.8\n",
      "Iteration: 7, Average Returns: 184.0\n",
      "Iteration: 8, Average Returns: 190.6\n",
      "Iteration: 9, Average Returns: 208.0\n",
      "Iteration: 10, Average Returns: 161.0\n",
      "Iteration: 11, Average Returns: 141.0\n",
      "Iteration: 12, Average Returns: 166.8\n",
      "Iteration: 13, Average Returns: 355.4\n",
      "Iteration: 14, Average Returns: 279.6\n",
      "Iteration: 15, Average Returns: 243.6\n",
      "Iteration: 16, Average Returns: 464.8\n",
      "Iteration: 17, Average Returns: 500.0\n",
      "Iteration: 18, Average Returns: 284.2\n",
      "Iteration: 19, Average Returns: 500.0\n",
      "Iteration: 20, Average Returns: 500.0\n",
      "Iteration: 21, Average Returns: 500.0\n",
      "Iteration: 22, Average Returns: 500.0\n",
      "Iteration: 23, Average Returns: 500.0\n",
      "Iteration: 24, Average Returns: 500.0\n",
      "Iteration: 25, Average Returns: 500.0\n",
      "Iteration: 26, Average Returns: 375.4\n",
      "Iteration: 27, Average Returns: 227.8\n",
      "Iteration: 28, Average Returns: 500.0\n",
      "Iteration: 29, Average Returns: 331.8\n",
      "Iteration: 30, Average Returns: 500.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "ray.init()\n",
    "tf.random.set_seed(seed=0)\n",
    "random.seed(0)\n",
    "agent = ApexDQN(env=CartPoleEnv, config={\n",
    "    'env_config': {'verbose': False},\n",
    "    'num_workers': 4,\n",
    "    'replay_buffer_config' : {\n",
    "        \"type\": 'MultiAgentPrioritizedReplayBuffer',\n",
    "        \"capacity\": 100000,\n",
    "        'prioritized_replay': True,\n",
    "        'prioritized_replay_alpha': 0.6,\n",
    "        'prioritized_replay_beta': 0.4,\n",
    "        'prioritized_replay_eps': 1e-6,\n",
    "        'replay_sequence_length': 1,\n",
    "    },\n",
    "    'num_steps_sampled_before_learning_starts': 10000,\n",
    "    'target_network_update_freq': 10000,\n",
    "    'rollout_fragment_length': 4,\n",
    "    'train_batch_size': 256,\n",
    "    'n_step': 3,\n",
    "    'double_q': True,\n",
    "    'dueling': True,\n",
    "    'noisy': True,\n",
    "    'num_atoms': 51,\n",
    "    'v_min': -500.0,\n",
    "    'v_max': 500.0,\n",
    "    'exploration_config': {\n",
    "        'epsilon_timesteps': 2,\n",
    "        'final_epsilon': 0.0\n",
    "    },\n",
    "    'seed': 0,\n",
    "    'gamma': 0.99,\n",
    "    'lr': 0.0005,\n",
    "    'num_gpus': 1\n",
    "})\n",
    "agent.get_policy().model.base_model.summary(expand_nested=True)\n",
    "utils.train(agent=agent, eval_env=CartPoleEnv(env_config={'verbose': False}))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  Custom Model Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ray.shutdown()\n",
    "ray.init()\n",
    "ModelCatalog.register_custom_model(\"dqn_model\", DQNCustomModel)\n",
    "tf.random.set_seed(seed=0)\n",
    "random.seed(0)\n",
    "agent = ApexDQN(env=CartPoleEnv, config={\n",
    "    'env_config': {'verbose': False},\n",
    "    'num_workers': 4,\n",
    "    'model': {\n",
    "        'custom_model': 'dqn_model',\n",
    "        'custom_model_config': {}\n",
    "    },\n",
    "    'replay_buffer_config' : {\n",
    "        \"type\": 'MultiAgentPrioritizedReplayBuffer',\n",
    "        \"capacity\": 100000,\n",
    "        'prioritized_replay': True,\n",
    "        'prioritized_replay_alpha': 0.6,\n",
    "        'prioritized_replay_beta': 0.4,\n",
    "        'prioritized_replay_eps': 1e-6,\n",
    "        'replay_sequence_length': 1,\n",
    "    },\n",
    "    'num_steps_sampled_before_learning_starts': 10000,\n",
    "    'target_network_update_freq': 10000,\n",
    "    'rollout_fragment_length': 4,\n",
    "    'train_batch_size': 256,\n",
    "    'n_step': 3,\n",
    "    'double_q': True,\n",
    "    'dueling': True,\n",
    "    'noisy': True,\n",
    "    'num_atoms': 51,\n",
    "    'v_min': -500.0,\n",
    "    'v_max': 500.0,\n",
    "    'exploration_config': {\n",
    "        'epsilon_timesteps': 2,\n",
    "        'final_epsilon': 0.0\n",
    "    },\n",
    "    'seed': 0,\n",
    "    'gamma': 0.99,\n",
    "    'lr': 0.0005,\n",
    "    'num_gpus': 1\n",
    "})\n",
    "agent.get_policy().model.base_model.summary(expand_nested=True)\n",
    "utils.train(agent=agent, eval_env=CartPoleEnv(env_config={'verbose': False}))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
